# Решение: Интеграция On-Device LLM с MediaPipe

## Итоговое решение

Успешно реализована интеграция on-device LLM (Gemma 2B) в Android приложение с использованием MediaPipe LLM Inference API. Модель работает полностью на устройстве без подключения к интернету.

## Архитектура решения

### Компоненты

1. **ModelManager** (`app/src/main/java/com/epicnotes/chat/data/llm/ModelManager.kt`)
   - Управляет жизненным циклом модели MediaPipe
   - Копирует модель из assets в internal storage (для больших моделей >2GB)
   - Обрабатывает таймауты, ошибки памяти и проверяет целостность файлов
   - Мониторинг производительности через `LlmMetrics`

2. **OnDeviceLlmClient** (`app/src/main/java/com/epicnotes/chat/data/llm/OnDeviceLlmClient.kt`)
   - Реализует интерфейс `LlmClient` для MediaPipe
   - Обрабатывает инференс с таймаутами и валидацией промптов

3. **LlmMetrics** (`app/src/main/java/com/epicnotes/chat/data/llm/LlmMetrics.kt`)
   - Отслеживает производительность: время загрузки, скорость инференса, использование памяти
   - Проверяет возможности устройства перед инициализацией

4. **UI компоненты**
   - `ChatScreen.kt`: Отображает прогресс загрузки модели и генерации ответа
   - `ChatViewModel.kt`: Управляет состоянием чата и взаимодействием с LLM

## Ключевые технические решения

### 1. Обработка больших моделей (>2GB)

**Проблема**: APK имеет ограничение ZIP32 (4GB), большие модели не могут быть включены в APK.

**Решение**:
- Исключение больших `.task` файлов из APK в `app/build.gradle.kts`:
  ```kotlin
  android {
      packaging {
          resources {
              excludes += listOf("**/models/*.task")
          }
      }
  }
  ```
- Скрипт `scripts/push_model.sh` для ручной загрузки модели на устройство
- `ModelManager` автоматически проверяет наличие модели в internal storage и копирует из assets, если модель небольшая

### 2. Работа с MediaPipe

**Проблема**: MediaPipe требует абсолютный путь к файлу, не может работать напрямую с assets.

**Решение**:
- Копирование модели из assets в `context.filesDir/models/` при первом запуске
- Использование абсолютного пути для MediaPipe API
- Проверка целостности файла (размер, заголовок) перед инициализацией

### 3. Обработка ошибок и таймауты

**Реализовано**:
- Таймаут инициализации: 60 секунд
- Таймаут инференса: 30 секунд
- Обработка `OutOfMemoryError` с понятными сообщениями
- Проверка возможностей устройства перед загрузкой модели
- Детальное логирование ошибок для диагностики

### 4. Версия MediaPipe

**Важно**: Используется MediaPipe `0.10.16` для совместимости с Gemma2 моделями. Более старые версии могут вызывать нативные краши при инициализации.

## Структура файлов

```
app/src/main/
├── assets/
│   └── models/
│       └── gemma2-2b-it-cpu-int8.task  # Модель (исключена из APK если >2GB)
├── java/com/epicnotes/chat/
│   ├── data/llm/
│   │   ├── ModelManager.kt          # Управление моделью
│   │   ├── OnDeviceLlmClient.kt     # Клиент для MediaPipe
│   │   ├── LlmConfig.kt             # Конфигурация LLM
│   │   └── LlmMetrics.kt            # Метрики производительности
│   └── presentation/
│       └── chat/
│           ├── ChatScreen.kt        # UI чата
│           ├── ChatViewModel.kt     # Логика чата
│           └── ChatUiState.kt       # Состояние UI
scripts/
└── push_model.sh                    # Скрипт для загрузки модели
```

## Использование

### Первая установка

1. **Скачать модель** (если >2GB):
   - Скачать `gemma2-2b-it-cpu-int8.task` с Kaggle или другого источника
   - Поместить в `app/src/main/assets/models/`

2. **Загрузить модель на устройство** (если модель >2GB):
   ```bash
   ./scripts/push_model.sh
   ```

3. **Собрать и установить приложение**:
   ```bash
   ./gradlew installDebug
   ```

4. **Запустить приложение**:
   ```bash
   adb shell am start -n com.epicnotes.chat/.MainActivity
   ```

### Использование в приложении

1. При первом сообщении модель автоматически загрузится (5-10 секунд)
2. Последующие сообщения обрабатываются быстрее (2-5 секунд)
3. Прогресс отображается в UI с оценкой времени

## Конфигурация

Настройки модели в `LlmConfig.kt`:

```kotlin
data class LlmConfig(
    val modelPath: String = "models/gemma2-2b-it-cpu-int8.task",
    val maxTokens: Int = 512,
    val temperature: Float = 0.7f,
    val topK: Int = 40,
    val randomSeed: Int = 42,
    val inferenceTimeoutMs: Long = 30_000L,
    val enableGpuAcceleration: Boolean = false,
    val minRequiredRamMB: Int = 3072,
    val enableMetrics: Boolean = true
)
```

## Производительность

- **Загрузка модели**: 5-10 секунд (первый запуск)
- **Генерация ответа**: 2-5 секунд (зависит от длины)
- **Использование памяти**: ~3-4GB RAM
- **Размер модели**: ~3GB (gemma2-2b-it-cpu-int8)

## Требования к устройству

- **RAM**: минимум 4GB (рекомендуется 6GB+)
- **Хранилище**: 3GB свободного места
- **Android**: API 26+ (Android 8.0+)
- **Архитектура**: ARM64 (arm64-v8a)

## Зависимости

В `gradle/libs.versions.toml`:
```toml
mediapipe = "0.10.16"
```

В `app/build.gradle.kts`:
```kotlin
implementation("com.google.mediapipe:tasks-genai:${libs.versions.mediapipe.get()}")
```

## Решенные проблемы

1. ✅ **APK ZIP32 limit**: Исключение больших моделей из APK
2. ✅ **MediaPipe путь к файлу**: Копирование в internal storage
3. ✅ **Нативные краши**: Обновление MediaPipe до 0.10.16
4. ✅ **Таймауты**: Защита от зависаний при инициализации и инференсе
5. ✅ **Обработка памяти**: Мониторинг и обработка OOM ошибок
6. ✅ **UI обратная связь**: Прогресс-индикаторы и оценка времени

## Дальнейшие улучшения

- [ ] Поддержка GPU ускорения (если доступно)
- [ ] Кэширование промптов для повторных запросов
- [ ] Оптимизация памяти для устройств с ограниченным RAM
- [ ] Поддержка streaming ответов (показ частичного текста во время генерации)
- [ ] Выбор модели в настройках (int4 vs int8, разные размеры)

## Заключение

Решение полностью функционально и готово к использованию. Модель работает on-device без подключения к интернету, с хорошей производительностью и надежной обработкой ошибок.
